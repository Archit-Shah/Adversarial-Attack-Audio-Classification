{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Dataset\n",
    "\n",
    "Download the Urban8K dataset from the link below:\n",
    "\n",
    "https://urbansounddataset.weebly.com/download-urbansound8k.html\n",
    "\n",
    "The page will ask for basic details and then direct to the download link. The data size is about 6GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Unzip the Dataset\n",
    "\n",
    "Once downloaded, unzip the file in \"..\\data\\unzipped\\\" folder path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Spectrogram Folder\n",
    "\n",
    "Create a folder to store spectrogram images for various puposes.\n",
    "\n",
    "1. Create folder path \"..\\spectogram_images\\train\"\n",
    "2. Create folder path \"..\\spectogram_images\\test\"\n",
    "3. Create folder path \"..\\spectogram_images\\adv_train_in\"\n",
    "4. Create folder path \"..\\spectogram_images\\adv_test_in\"\n",
    "5. Create folder path \"..\\spectogram_images\\adv_train_out\"\n",
    "6. Create folder path \"..\\spectogram_images\\adv_test_out\"\n",
    "7. Create folder path \"..\\spectrogram_images\\train_adv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import librosa\n",
    "from librosa import display\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pylab\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from shutil import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import progressbar\n",
    "from time import sleep\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store \n",
      ".DS_Store \n",
      ".DS_Store \n",
      ".DS_Store \n",
      ".DS_Store \n"
     ]
    }
   ],
   "source": [
    "#Preprocessing Files in Batch to generate and save Spectogram images\n",
    "\n",
    "WAV_DIR = 'data/unzipped/UrbanSound8K/audio/fold'\n",
    "IMG_DIR = 'spectrogram_images/train/'\n",
    "I_ADV_TRAIN_DIR = 'spectrogram_images/adv_train_in/'\n",
    "I_ADV_TEST_DIR = 'spectrogram_images/adv_test_in/'\n",
    "O_ADV_TRAIN_DIR = 'spectrogram_images/adv_train_out/'\n",
    "O_ADV_TEST_DIR = 'spectrogram_images/adv_test_out/'\n",
    "\n",
    "ADV_TRAIN_DIR = 'spectrogram_images/train_adv/'\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(1,4):\n",
    "for i in range(1,9):\n",
    "    wav_files = os.listdir(WAV_DIR + str(i) + '/')\n",
    "    for f in wav_files:\n",
    "        try:\n",
    "            # Read wav-file\n",
    "            y, sr = librosa.load(WAV_DIR + str(i) + '/' + f, sr = 22050)\n",
    "\n",
    "            # Compute spectrogram\n",
    "            M = librosa.feature.melspectrogram(y, sr, fmax = sr/2, n_fft=2048, hop_length=512, n_mels = 96, power = 2)\n",
    "\n",
    "            # Power in DB\n",
    "            log_power = librosa.power_to_db(M, ref=np.max)\n",
    "\n",
    "            # Plotting the spectrogram\n",
    "            pylab.figure(figsize=(5,5))\n",
    "            pylab.axis('off') \n",
    "            pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[])\n",
    "            librosa.display.specshow(log_power, cmap=cm.jet)\n",
    "            pylab.savefig(IMG_DIR + f[:-4]+'.jpg', bbox_inches=None, pad_inches=0)\n",
    "            pylab.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f, e)\n",
    "            pass\n",
    "\n",
    "for i in range(9,10):\n",
    "    wav_files = os.listdir(WAV_DIR + str(i) + '/')\n",
    "    for f in wav_files:\n",
    "        try:\n",
    "            # Read wav-file\n",
    "            y, sr = librosa.load(WAV_DIR + str(i) + '/' + f, sr = 22050)\n",
    "\n",
    "            # Compute spectrogram\n",
    "            M = librosa.feature.melspectrogram(y, sr, fmax = sr/2, n_fft=2048, hop_length=512, n_mels = 96, power = 2)\n",
    "\n",
    "            # Power in DB\n",
    "            log_power = librosa.power_to_db(M, ref=np.max)\n",
    "\n",
    "            # Plotting the spectrogram\n",
    "            pylab.figure(figsize=(5,5))\n",
    "            pylab.axis('off') \n",
    "            pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[])\n",
    "            librosa.display.specshow(log_power, cmap=cm.jet)\n",
    "            pylab.savefig(I_ADV_TRAIN_DIR + f[:-4]+'.jpg', bbox_inches=None, pad_inches=0)\n",
    "            pylab.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f, e)\n",
    "            pass\n",
    "        \n",
    "for i in range(10,11):\n",
    "    wav_files = os.listdir(WAV_DIR + str(i) + '/')\n",
    "    for f in wav_files:\n",
    "        try:\n",
    "            # Read wav-file\n",
    "            y, sr = librosa.load(WAV_DIR + str(i) + '/' + f, sr = 22050)\n",
    "\n",
    "            # Compute spectrogram\n",
    "            M = librosa.feature.melspectrogram(y, sr, fmax = sr/2, n_fft=2048, hop_length=512, n_mels = 96, power = 2)\n",
    "\n",
    "            # Power in DB\n",
    "            log_power = librosa.power_to_db(M, ref=np.max)\n",
    "\n",
    "            # Plotting the spectrogram\n",
    "            pylab.figure(figsize=(5,5))\n",
    "            pylab.axis('off') \n",
    "            pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[])\n",
    "            librosa.display.specshow(log_power, cmap=cm.jet)\n",
    "            pylab.savefig(I_ADV_TEST_DIR + f[:-4]+'.jpg', bbox_inches=None, pad_inches=0)\n",
    "            pylab.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f, e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = 'spectrogram_images/train/'\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 16\n",
    "BATCH_SIZE = 10\n",
    "L2_LAMBDA = 0.001\n",
    "IMG_SIZE = (224, 224)\n",
    "WAV_DIR = 'data/unzipped/UrbanSound8K/audio/fold'\n",
    "IMG_DIR = 'spectrogram_images/train/'\n",
    "I_ADV_TRAIN_DIR = 'spectrogram_images/adv_train_in/'\n",
    "I_ADV_TEST_DIR = 'spectrogram_images/adv_test_in/'\n",
    "O_ADV_TRAIN_DIR = 'spectrogram_images/adv_train_out/'\n",
    "O_ADV_TEST_DIR = 'spectrogram_images/adv_test_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = OneHotEncoder(categories=[range(NUM_CLASSES)])\n",
    "\n",
    "all_files = os.listdir(IMG_DIR)\n",
    "\n",
    "# Get class weights\n",
    "label_array = []\n",
    "for file_ in all_files:\n",
    "    vals = file_.split('-')[1]\n",
    "    label_array.append(int(vals))\n",
    "    \n",
    "cl_weight = compute_class_weight(class_weight = 'balanced', \n",
    "                                 classes = np.unique(label_array), \n",
    "                                 y = label_array)\n",
    "\n",
    "# Train-val-test split of files\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(all_files, \n",
    "                                                                      label_array,\n",
    "                                                                      random_state = 10, \n",
    "                                                                      test_size = 0.2\n",
    "                                                                     )\n",
    "\n",
    "# Among the test files, keep half for validation\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(test_files, test_labels,\n",
    "                                                                  random_state = 10, \n",
    "                                                                  test_size = 0.5\n",
    "                                                                 )\n",
    "\n",
    "adv_train_files = os.listdir(I_ADV_TRAIN_DIR)\n",
    "\n",
    "# Get class weights\n",
    "adv_train_label = []\n",
    "for file_ in all_files:\n",
    "    vals = file_.split('-')[1]\n",
    "    adv_train_label.append(int(vals))\n",
    "\n",
    "adv_test_files = os.listdir(I_ADV_TEST_DIR)\n",
    "\n",
    "# Get class weights\n",
    "adv_test_label = []\n",
    "for file_ in all_files:\n",
    "    vals = file_.split('-')[1]\n",
    "    adv_test_label.append(int(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = transforms.Compose([transforms.Resize(IMG_SIZE), transforms.ToTensor()])\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def image_loader(image_name):\n",
    "    \"\"\"load image, returns tensor\"\"\"\n",
    "    image = Image.open(image_name)\n",
    "    image = image.convert(\"RGB\")\n",
    "    image = loader(image).float()\n",
    "    image = normalize(image).float()\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_batch(file_list):\n",
    "    img_array = []\n",
    "    idx_array = []\n",
    "    label_array = []\n",
    "\n",
    "    for file_ in file_list:\n",
    "        im = IMG_DIR + file_\n",
    "        \n",
    "        img_array.append(np.array(image_loader(im)))\n",
    "\n",
    "        vals = file_.split('-')\n",
    "        idx_array.append(vals[0])\n",
    "        label_array.append([int(vals[1])])\n",
    "\n",
    "    label_array = one_hot.fit_transform(label_array).toarray()\n",
    "    img_array = torch.FloatTensor(img_array)\n",
    "    label_array = torch.LongTensor(label_array)\n",
    "    \n",
    "    return img_array, label_array\n",
    "\n",
    "def batch_generator(files, BATCH_SIZE):\n",
    "    L = len(files)\n",
    "\n",
    "    #this line is just to make the generator infinite, keras needs that    \n",
    "    while True:\n",
    "\n",
    "        batch_start = 0\n",
    "        batch_end = BATCH_SIZE\n",
    "\n",
    "        while batch_start < L:\n",
    "            \n",
    "            limit = min(batch_end, L)\n",
    "            file_list = files[batch_start: limit]\n",
    "            batch_img_array, batch_label_array = load_batch(file_list)\n",
    "\n",
    "            yield (batch_img_array, batch_label_array) # a tuple with two numpy arrays with batch_size samples     \n",
    "\n",
    "            batch_start += BATCH_SIZE   \n",
    "            batch_end += BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3)\n",
      "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Model Architecture Definition\n",
    "\n",
    "model = models.vgg16(pretrained=True) #load vgg model\n",
    "\n",
    "# you need to flatten the layer after vgg16 to get 18432, check the keras model in the original nb\n",
    "model.classifier = nn.Sequential(nn.Linear(512*7*7,512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.3),\n",
    "                                nn.Linear(512,NUM_CLASSES))  \n",
    "# note there is no softmax layer while training, validating and testing put in that operation manually as the learning\n",
    "# doesn't depend upon softmax layer, this way we will get logits. We can use logits in fgsm\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Validation Functions\n",
    "\n",
    "def train (model, loader, criterion):\n",
    "    #Custom Code for Progress Bar\n",
    "    bar = progressbar.ProgressBar(maxval=100, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    model.train()\n",
    "    current_loss = 0\n",
    "    current_correct = 0\n",
    "    i = 0\n",
    "    i_percentage = 0\n",
    "    for train, y_train in iter(loader):\n",
    "        i = i+train.shape[0]\n",
    "        if i > len(train_files):\n",
    "            break\n",
    "        y_train = torch.argmax(y_train, dim=1)\n",
    "        train, y_train = train.to(device), y_train.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(train)\n",
    "        _, preds = torch.max(output,1)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.classifier.parameters(), 50)\n",
    "        optimizer.step()\n",
    "        current_loss += loss.item()*train.size(0)\n",
    "        current_correct += torch.sum(preds == y_train.data)\n",
    "        i_percentage = (i/len(train_files))*100\n",
    "        bar.update(i_percentage)\n",
    "        \n",
    "    bar.finish()\n",
    "    epoch_loss = current_loss / len(train_files)\n",
    "    epoch_acc = current_correct.double() / len(train_files)\n",
    "        \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "#validate the model\n",
    "def validation (model, loader, criterion):\n",
    "    #Custom Code for Progress Bar\n",
    "    bar = progressbar.ProgressBar(maxval=100, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_correct = 0\n",
    "    i = 0\n",
    "    i_percentage = 0\n",
    "    for valid, y_valid in iter(loader):\n",
    "        i = i+valid.shape[0]\n",
    "        if i > len(val_files):\n",
    "            break\n",
    "        y_valid = torch.argmax(y_valid, dim=1)\n",
    "        valid, y_valid = valid.to(device), y_valid.to(device)\n",
    "        output = model.forward(valid)\n",
    "        valid_loss += criterion(output, y_valid).item()*valid.size(0)\n",
    "        equal = (output.max(dim=1)[1] == y_valid.data)\n",
    "        valid_correct += torch.sum(equal)#type(torch.FloatTensor)\n",
    "        i_percentage = (i/len(val_files))*100\n",
    "        bar.update(i_percentage)\n",
    "        \n",
    "    bar.finish()\n",
    "    epoch_loss = valid_loss / len(val_files)\n",
    "    epoch_acc = valid_correct.double() / len(val_files)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def testing (model, loader, criterion, test_size):\n",
    "    #Custom Code for Progress Bar\n",
    "    bar = progressbar.ProgressBar(maxval=100, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    i = 0\n",
    "    i_percentage = 0\n",
    "    for test, y_test in iter(loader):\n",
    "        i = i+test.shape[0]\n",
    "        if i > len(test_files):\n",
    "            break\n",
    "        y_test = torch.argmax(y_test, dim=1)\n",
    "        test, y_test = test.to(device), y_test.to(device)\n",
    "        output = model.forward(test)\n",
    "        test_loss += criterion(output, y_test).item()*test.size(0)\n",
    "        equal = (output.max(dim=1)[1] == y_test.data)\n",
    "        test_correct += torch.sum(equal)#type(torch.FloatTensor)\n",
    "        i_percentage = (i/test_size)*100\n",
    "        bar.update(i_percentage)\n",
    "        \n",
    "    bar.finish()\n",
    "    epoch_loss = test_loss / test_size\n",
    "    epoch_acc = test_correct.double() / test_size\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr = 0.005, momentum = 0.5)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss : 0.0334  Train Accuracy: 0.9951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Validation Loss : 0.3878  Validation Accuracy 0.8941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss : 0.0270  Train Accuracy: 0.9958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Validation Loss : 0.3563  Validation Accuracy 0.9025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss : 0.0222  Train Accuracy: 0.9958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Validation Loss : 0.3867  Validation Accuracy 0.8997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss : 0.0158  Train Accuracy: 0.9988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Validation Loss : 0.3999  Validation Accuracy 0.8955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Loss : 0.0161  Train Accuracy: 0.9974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Validation Loss : 0.3619  Validation Accuracy 0.9025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train Loss : 0.0130  Train Accuracy: 0.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Validation Loss : 0.3884  Validation Accuracy 0.8856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train Loss : 0.0096  Train Accuracy: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Validation Loss : 0.3770  Validation Accuracy 0.9025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train Loss : 0.0095  Train Accuracy: 0.9984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Validation Loss : 0.3857  Validation Accuracy 0.9011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train Loss : 0.0075  Train Accuracy: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Validation Loss : 0.3555  Validation Accuracy 0.9110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss : 0.0093  Train Accuracy: 0.9981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Validation Loss : 0.3711  Validation Accuracy 0.9025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Train Loss : 0.0069  Train Accuracy: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Validation Loss : 0.3727  Validation Accuracy 0.9054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Train Loss : 0.0071  Train Accuracy: 0.9986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Validation Loss : 0.3594  Validation Accuracy 0.9110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Train Loss : 0.0061  Train Accuracy: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Validation Loss : 0.3605  Validation Accuracy 0.9096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Train Loss : 0.0053  Train Accuracy: 0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Validation Loss : 0.3764  Validation Accuracy 0.9040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Train Loss : 0.0048  Train Accuracy: 0.9996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Validation Loss : 0.3552  Validation Accuracy 0.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Train Loss : 0.0040  Train Accuracy: 0.9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Validation Loss : 0.3546  Validation Accuracy 0.9195\n",
      "-4408.3645240000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#Train the model\n",
    "\n",
    "#freeze gradient parameters in pretrained model\n",
    "for param in model.parameters():\n",
    "    param.require_grad = True    \n",
    "\n",
    "#freeze gradient parameters in pretrained model\n",
    "for param in model.features.parameters():\n",
    "    param.require_grad = False\n",
    "    \n",
    "#train and validate\n",
    "#4 epochs seem to be enough\n",
    "epochs = 4\n",
    "epoch = 0\n",
    "\n",
    "start = time.clock()    \n",
    "for e in range(epochs):\n",
    "    epoch +=1\n",
    "    with torch.set_grad_enabled(True):\n",
    "        epoch_train_loss, epoch_train_acc = train(model,batch_generator(train_files,BATCH_SIZE),criteria)\n",
    "    print(\"Epoch: {} Train Loss : {:.4f}  Train Accuracy: {:.4f}\".format(epoch,epoch_train_loss,epoch_train_acc))\n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss, epoch_val_acc = validation(model,batch_generator(val_files,BATCH_SIZE),criteria)\n",
    "    print(\"Epoch: {} Validation Loss : {:.4f}  Validation Accuracy {:.4f}\".format(epoch,epoch_val_loss,epoch_val_acc))\n",
    "print(start - time.clock())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the original training model\n",
    "torch.save(model,'original_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('original_model.pt',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Test Loss : 0.4645  Test Accuracy 0.8785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Test Results\n",
    "\n",
    "with torch.no_grad():\n",
    "    epoch_test_loss, epoch_test_acc = testing(model,batch_generator(test_files,BATCH_SIZE),criteria,len(test_files))\n",
    "print(\"Epoch: {} Test Loss : {:.4f}  Test Accuracy {:.4f}\".format(1,epoch_test_loss,epoch_test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'spectrogram_images/train/103249-5-0-15.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-33dc0988cf9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0madv_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madv_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_train_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_org\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-65c7f673e6ce>\u001b[0m in \u001b[0;36mload_batch\u001b[0;34m(file_list)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIMG_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mimg_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-65c7f673e6ce>\u001b[0m in \u001b[0;36mimage_loader\u001b[0;34m(image_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\"load image, returns tensor\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/lib/python3.5/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spectrogram_images/train/103249-5-0-15.jpg'"
     ]
    }
   ],
   "source": [
    "def load_batch_2(file_list):\n",
    "    img_array = []\n",
    "    idx_array = []\n",
    "    label_array = []\n",
    "\n",
    "    for file_ in file_list:\n",
    "        im = I_ADV_TRAIN_DIR + file_\n",
    "        \n",
    "        img_array.append(np.array(image_loader(im)))\n",
    "\n",
    "        vals = file_.split('-')\n",
    "        idx_array.append(vals[0])\n",
    "        label_array.append([int(vals[1])])\n",
    "\n",
    "    label_array = one_hot.fit_transform(label_array).toarray()\n",
    "    img_array = torch.FloatTensor(img_array)\n",
    "    label_array = torch.LongTensor(label_array)\n",
    "    \n",
    "    return img_array, label_array\n",
    "\n",
    "\n",
    "#Generate Adversarial Images\n",
    "batch_size = 5\n",
    "pred = []\n",
    "adv_pred = []\n",
    "if 'adv_x' in locals():\n",
    "    del adv_x\n",
    "\n",
    "i=0\n",
    "bar = progressbar.ProgressBar(maxval=100, \\\n",
    "widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "bar.start()\n",
    "\n",
    "un_normalize = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],std=[1/0.229, 1/0.224, 1/0.225])\n",
    "un_convert = transforms.ToPILImage()\n",
    "\n",
    "# ADV_IMG_DIR = 'spectrogram_images/adv/'\n",
    "\n",
    "start = time.time()\n",
    "adv_size = len(adv_train_files)\n",
    "file_idx = 0\n",
    "while i<adv_size:\n",
    "    j = i + batch_size\n",
    "    if j>adv_size:\n",
    "        j = adv_size\n",
    "    x,y = load_batch_2(adv_train_files[i:j])\n",
    "    x = Variable(x, requires_grad=True)\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    \n",
    "    output = model.forward(x)\n",
    "    y_label = torch.argmax(y, dim=1)\n",
    "    y_label = y_label.to(device)\n",
    "    loss = criteria(output, y_label)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add perturbation \n",
    "    epsilon = 0.01\n",
    "    x_grad     = torch.sign(x.grad.data)\n",
    "    temp = x.data + epsilon*x_grad\n",
    "    if 'adv_x' in locals():\n",
    "        adv_x = torch.cat((adv_x,temp),0)\n",
    "    else:\n",
    "        adv_x = temp\n",
    "    adv_x = adv_x.to(device)\n",
    "    adv_output = model.forward(adv_x)\n",
    "    adv_output = adv_output.to(device)\n",
    "\n",
    "    x_pred = torch.argmax(y, dim=1)\n",
    "    x_adv_pred = torch.argmax(adv_output,dim=1)\n",
    "    \n",
    "    x_pred, x_adv_pred = x_pred.to(device), x_adv_pred.to(device)\n",
    "\n",
    "    pred.extend(x_pred.tolist())\n",
    "    adv_pred.extend(x_adv_pred.tolist())\n",
    "    for ii in range(j-i):\n",
    "        temp = adv_x[ii]\n",
    "        t1 = un_normalize(temp).float()\n",
    "        t2 = un_convert(t1)\n",
    "        t2.save(O_ADV_TRAIN_DIR + adv_train_files[file_idx], \"JPEG\", quality=80, optimize=True, progressive=True)\n",
    "        file_idx += 1\n",
    "        del temp\n",
    "        del t1\n",
    "        del t2\n",
    "    i = j\n",
    "    del adv_x\n",
    "    bar.update((i/adv_size)*100)\n",
    "\n",
    "bar.finish()\n",
    "\n",
    "comp = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == adv_pred[i]:\n",
    "        comp.extend([1])\n",
    "    else:\n",
    "        comp.extend([0])\n",
    "\n",
    "adv_acc = (sum(comp)/len(comp))*100\n",
    "print(\"The accuracy of model with adversarial examples is \",\"{0:.2f}\".format(adv_acc),\"%\")\n",
    "print(start - time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the files from training and adversarial images folder into a single training folder\n",
    "ADV_TRAIN_DIR = 'spectrogram_images/train_adv/'\n",
    "\n",
    "for f in train_files:\n",
    "    copy(IMG_DIR + f,ADV_TRAIN_DIR)\n",
    "\n",
    "for f in adv_train_files:\n",
    "    copy(O_ADV_TRAIN_DIR + f,ADV_TRAIN_DIR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the network now with original + adversarial images\n",
    "adv_files = os.listdir(ADV_TRAIN_DIR)\n",
    "\n",
    "# Get class weights\n",
    "adv_label = []\n",
    "for file_ in all_files:\n",
    "    vals = file_.split('-')[1]\n",
    "    adv_label.append(int(vals))\n",
    "    \n",
    "cl_weight = compute_class_weight(class_weight = 'balanced', \n",
    "                                 classes = np.unique(label_array), \n",
    "                                 y = label_array)\n",
    "\n",
    "# Train-val-test split of files\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(adv_files, \n",
    "                                                                      adv_label,\n",
    "                                                                      random_state = 10, \n",
    "                                                                      test_size = 0.2\n",
    "                                                                     )\n",
    "\n",
    "# Among the test files, keep half for validation\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(test_files, test_labels,\n",
    "                                                                  random_state = 10, \n",
    "                                                                  test_size = 0.5\n",
    "                                                                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_train (model, loader, criterion):\n",
    "    #Custom Code for Progress Bar\n",
    "    bar = progressbar.ProgressBar(maxval=100, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    model.train()\n",
    "    current_loss = 0\n",
    "    current_correct = 0\n",
    "    i = 0\n",
    "    i_percentage = 0\n",
    "    for train, y_train in iter(loader):\n",
    "        train = Variable(train, requires_grad=True)\n",
    "        i = i+train.shape[0]\n",
    "        if i > len(train_files):\n",
    "            break\n",
    "        y_train = torch.argmax(y_train, dim=1)\n",
    "        train, y_train = train.to(device), y_train.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(train)\n",
    "        _, preds = torch.max(output,1)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "#         #adversarial\n",
    "        \n",
    "#         output = model.forward(x)\n",
    "#         y_label = torch.argmax(y, dim=1)\n",
    "#         loss = criteria(output, y_label)\n",
    "#         loss.backward()\n",
    "\n",
    "        # Add perturbation \n",
    "        epsilon = 0.01\n",
    "        train_grad = torch.sign(train.grad.data)\n",
    "        adv_train = train.data + epsilon*train_grad\n",
    "        adv_train = adv_train.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        adv_output = model.forward(adv_train)\n",
    "        adv_output = adv_output.to(device)\n",
    "        _, preds = torch.max(adv_output,1)\n",
    "        loss = 0.6 * (loss) + 0.4 * (criterion(adv_output, y_train))\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.classifier.parameters(), 50)\n",
    "        optimizer.step()\n",
    "        current_loss += loss.item()*train.size(0)\n",
    "        current_correct += torch.sum(preds == y_train.data)\n",
    "        i_percentage = (i/len(train_files))*100\n",
    "        bar.update(i_percentage)\n",
    "        \n",
    "    bar.finish()\n",
    "    epoch_loss = current_loss / len(train_files)\n",
    "    epoch_acc = current_correct.double() / len(train_files)\n",
    "        \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Model Architecture Definition\n",
    "\n",
    "model = models.vgg16(pretrained=True) #load vgg model\n",
    "\n",
    "# you need to flatten the layer after vgg16 to get 18432, check the keras model in the original nb\n",
    "model.classifier = nn.Sequential(nn.Linear(512*7*7,512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.3),\n",
    "                                nn.Linear(512,NUM_CLASSES))  \n",
    "# note there is no softmax layer while training, validating and testing put in that operation manually as the learning\n",
    "# doesn't depend upon softmax layer, this way we will get logits. We can use logits in fgsm\n",
    "\n",
    "criteria = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr = 0.005, momentum = 0.5)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "#freeze gradient parameters in pretrained model\n",
    "for param in model.parameters():\n",
    "    param.require_grad = True    \n",
    "\n",
    "#freeze gradient parameters in pretrained model\n",
    "for param in model.features.parameters():\n",
    "    param.require_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_v(file_list):\n",
    "    img_array = []\n",
    "    idx_array = []\n",
    "    label_array = []\n",
    "\n",
    "    for file_ in file_list:\n",
    "        im = ADV_TRAIN_DIR + file_\n",
    "        \n",
    "        img_array.append(np.array(image_loader(im)))\n",
    "\n",
    "        vals = file_.split('-')\n",
    "        idx_array.append(vals[0])\n",
    "        label_array.append([int(vals[1])])\n",
    "\n",
    "    label_array = one_hot.fit_transform(label_array).toarray()\n",
    "    img_array = torch.FloatTensor(img_array)\n",
    "    label_array = torch.LongTensor(label_array)\n",
    "    \n",
    "    return img_array, label_array\n",
    "\n",
    "def batch_generator_v(files, BATCH_SIZE):\n",
    "    L = len(files)\n",
    "\n",
    "    #this line is just to make the generator infinite, keras needs that    \n",
    "    while True:\n",
    "\n",
    "        batch_start = 0\n",
    "        batch_end = BATCH_SIZE\n",
    "\n",
    "        while batch_start < L:\n",
    "            \n",
    "            limit = min(batch_end, L)\n",
    "            file_list = files[batch_start: limit]\n",
    "            batch_img_array, batch_label_array = load_batch(file_list)\n",
    "\n",
    "            yield (batch_img_array, batch_label_array) # a tuple with two numpy arrays with batch_size samples     \n",
    "\n",
    "            batch_start += BATCH_SIZE   \n",
    "            batch_end += BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and validate\n",
    "#Need to see at what epoch gain stops\n",
    "epochs = 5\n",
    "epoch = 0\n",
    "\n",
    "start = time.time()\n",
    "for e in range(epochs):\n",
    "    epoch +=1\n",
    "    print(epoch)\n",
    "    with torch.set_grad_enabled(True):\n",
    "        epoch_train_loss, epoch_train_acc = adv_train(model,batch_generator_v(train_files,BATCH_SIZE),criteria)\n",
    "    print(\"Epoch: {} Train Loss : {:.4f}  Train Accuracy: {:.4f}\".format(epoch,epoch_train_loss,epoch_train_acc))\n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss, epoch_val_acc = validation(model,batch_generator_v(val_files,BATCH_SIZE),criteria)\n",
    "    print(\"Epoch: {} Validation Loss : {:.4f}  Validation Accuracy {:.4f}\".format(epoch,epoch_val_loss,epoch_val_acc))\n",
    "print(start - time.time())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'adv_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adv = torch.load('adv_model.pt',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Results\n",
    "\n",
    "with torch.no_grad():\n",
    "    epoch_test_loss, epoch_test_acc = testing(model,batch_generator_v(test_files,BATCH_SIZE),criteria,len(test_files))\n",
    "print(\"Epoch: {} Test Loss : {:.4f}  Test Accuracy {:.4f}\".format(1,epoch_test_loss,epoch_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_3(file_list):\n",
    "    img_array = []\n",
    "    idx_array = []\n",
    "    label_array = []\n",
    "\n",
    "    for file_ in file_list:\n",
    "        im = I_ADV_TEST_DIR + file_\n",
    "        \n",
    "        img_array.append(np.array(image_loader(im)))\n",
    "\n",
    "        vals = file_.split('-')\n",
    "        idx_array.append(vals[0])\n",
    "        label_array.append([int(vals[1])])\n",
    "\n",
    "    label_array = one_hot.fit_transform(label_array).toarray()\n",
    "    img_array = torch.FloatTensor(img_array)\n",
    "    label_array = torch.LongTensor(label_array)\n",
    "    \n",
    "    return img_array, label_array\n",
    "\n",
    "start = time.time()\n",
    "#Generate Adversarial Images\n",
    "batch_size = 5\n",
    "pred = []\n",
    "adv_pred = []\n",
    "if 'adv_x' in locals():\n",
    "    del adv_x\n",
    "\n",
    "i=0\n",
    "bar = progressbar.ProgressBar(maxval=100, \\\n",
    "widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "bar.start()\n",
    "\n",
    "un_normalize = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],std=[1/0.229, 1/0.224, 1/0.225])\n",
    "un_convert = transforms.ToPILImage()\n",
    "\n",
    "# ADV_IMG_DIR = 'spectrogram_images/adv/'\n",
    "\n",
    "\n",
    "adv_size = len(adv_test_files)\n",
    "file_idx = 0\n",
    "while i<adv_size:\n",
    "    j = i + batch_size\n",
    "    if j>adv_size:\n",
    "        j = adv_size\n",
    "    x,y = load_batch_3(adv_test_files[i:j])\n",
    "    x = Variable(x, requires_grad=True)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    output = model.forward(x)\n",
    "    y_label = torch.argmax(y, dim=1)\n",
    "    y_label = y_label.to(device)\n",
    "    loss = criteria(output, y_label)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add perturbation \n",
    "    epsilon = 0.01\n",
    "    x_grad     = torch.sign(x.grad.data)\n",
    "    x_grad = x_grad.to(device)\n",
    "    temp = x.data + epsilon*x_grad\n",
    "    if 'adv_x' in locals():\n",
    "        adv_x = torch.cat((adv_x,temp),0)\n",
    "    else:\n",
    "        adv_x = temp\n",
    "    adv_x = adv_x.to(device)\n",
    "    adv_output = model.forward(adv_x)\n",
    "    adv_output = adv_output.to(device)\n",
    "\n",
    "    x_pred = torch.argmax(y, dim=1)\n",
    "    x_adv_pred = torch.argmax(adv_output,dim=1)\n",
    "    \n",
    "    x_pred, x_adv_pred = x_pred.to(device), x_adv_pred.to(device)\n",
    "\n",
    "    pred.extend(x_pred.tolist())\n",
    "    adv_pred.extend(x_adv_pred.tolist())\n",
    "    for ii in range(j-i):\n",
    "        temp = adv_x[ii]\n",
    "        t1 = un_normalize(temp).float()\n",
    "        t2 = un_convert(t1)\n",
    "        t2.save(O_ADV_TEST_DIR + adv_test_files[file_idx], \"JPEG\", quality=80, optimize=True, progressive=True)\n",
    "        file_idx += 1\n",
    "        del temp\n",
    "        del t1\n",
    "        del t2\n",
    "    i = j\n",
    "    del adv_x\n",
    "    bar.update((i/adv_size)*100)\n",
    "\n",
    "bar.finish()\n",
    "\n",
    "comp = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == adv_pred[i]:\n",
    "        comp.extend([1])\n",
    "    else:\n",
    "        comp.extend([0])\n",
    "\n",
    "adv_acc = (sum(comp)/len(comp))*100\n",
    "print(\"The accuracy of model with adversarial examples is \",\"{0:.2f}\".format(adv_acc),\"%\")\n",
    "print(start - time.time()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
