{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Dataset\n",
    "\n",
    "Download the Urban8K dataset from the link below:\n",
    "\n",
    "https://urbansounddataset.weebly.com/download-urbansound8k.html\n",
    "\n",
    "The page will ask for basic details and then direct to the download link. The data size is about 6GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Unzip the Dataset\n",
    "\n",
    "Once downloaded, unzip the file in \"..\\data\\unzipped\\\" folder path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Spectrogram Folder\n",
    "\n",
    "Create a folder to store spectrogram images for various puposes.\n",
    "\n",
    "1. Create folder path \"..\\spectogram_images\\train\"\n",
    "2. Create folder path \"..\\spectogram_images\\test\"\n",
    "3. Create folder path \"..\\spectogram_images\\adv_train_in\"\n",
    "4. Create folder path \"..\\spectogram_images\\adv_test_in\"\n",
    "5. Create folder path \"..\\spectogram_images\\adv_train_out\"\n",
    "6. Create folder path \"..\\spectogram_images\\adv_test_out\"\n",
    "7. Create folder path \"..\\spectrogram_images\\train_adv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import librosa\n",
    "from librosa import display\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pylab\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import os as os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from shutil import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import progressbar\n",
    "from time import sleep\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store \n",
      ".DS_Store \n",
      ".DS_Store \n",
      ".DS_Store \n",
      ".DS_Store \n"
     ]
    }
   ],
   "source": [
    "#Preprocessing Files in Batch to generate and save Spectogram images\n",
    "\n",
    "WAV_DIR = 'data/unzipped/UrbanSound8K/audio/fold'\n",
    "IMG_DIR = 'spectrogram_images/train/'\n",
    "I_ADV_TRAIN_DIR = 'spectrogram_images/adv_train_in/'\n",
    "I_ADV_TEST_DIR = 'spectrogram_images/adv_test_in/'\n",
    "O_ADV_TRAIN_DIR = 'spectrogram_images/adv_train_out/'\n",
    "O_ADV_TEST_DIR = 'spectrogram_images/adv_test_out/'\n",
    "\n",
    "ADV_TRAIN_DIR = 'spectrogram_images/train_adv/'\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(1,4):\n",
    "for i in range(1,9):\n",
    "    wav_files = os.listdir(WAV_DIR + str(i) + '/')\n",
    "    for f in wav_files:\n",
    "        try:\n",
    "            # Read wav-file\n",
    "            y, sr = librosa.load(WAV_DIR + str(i) + '/' + f, sr = 22050)\n",
    "\n",
    "            # Compute spectrogram\n",
    "            M = librosa.feature.melspectrogram(y, sr, fmax = sr/2, n_fft=2048, hop_length=512, n_mels = 96, power = 2)\n",
    "\n",
    "            # Power in DB\n",
    "            log_power = librosa.power_to_db(M, ref=np.max)\n",
    "\n",
    "            # Plotting the spectrogram\n",
    "            pylab.figure(figsize=(5,5))\n",
    "            pylab.axis('off') \n",
    "            pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[])\n",
    "            librosa.display.specshow(log_power, cmap=cm.jet)\n",
    "            pylab.savefig(IMG_DIR + f[:-4]+'.jpg', bbox_inches=None, pad_inches=0)\n",
    "            pylab.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f, e)\n",
    "            pass\n",
    "\n",
    "for i in range(9,10):\n",
    "    wav_files = os.listdir(WAV_DIR + str(i) + '/')\n",
    "    for f in wav_files:\n",
    "        try:\n",
    "            # Read wav-file\n",
    "            y, sr = librosa.load(WAV_DIR + str(i) + '/' + f, sr = 22050)\n",
    "\n",
    "            # Compute spectrogram\n",
    "            M = librosa.feature.melspectrogram(y, sr, fmax = sr/2, n_fft=2048, hop_length=512, n_mels = 96, power = 2)\n",
    "\n",
    "            # Power in DB\n",
    "            log_power = librosa.power_to_db(M, ref=np.max)\n",
    "\n",
    "            # Plotting the spectrogram\n",
    "            pylab.figure(figsize=(5,5))\n",
    "            pylab.axis('off') \n",
    "            pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[])\n",
    "            librosa.display.specshow(log_power, cmap=cm.jet)\n",
    "            pylab.savefig(I_ADV_TRAIN_DIR + f[:-4]+'.jpg', bbox_inches=None, pad_inches=0)\n",
    "            pylab.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f, e)\n",
    "            pass\n",
    "        \n",
    "for i in range(10,11):\n",
    "    wav_files = os.listdir(WAV_DIR + str(i) + '/')\n",
    "    for f in wav_files:\n",
    "        try:\n",
    "            # Read wav-file\n",
    "            y, sr = librosa.load(WAV_DIR + str(i) + '/' + f, sr = 22050)\n",
    "\n",
    "            # Compute spectrogram\n",
    "            M = librosa.feature.melspectrogram(y, sr, fmax = sr/2, n_fft=2048, hop_length=512, n_mels = 96, power = 2)\n",
    "\n",
    "            # Power in DB\n",
    "            log_power = librosa.power_to_db(M, ref=np.max)\n",
    "\n",
    "            # Plotting the spectrogram\n",
    "            pylab.figure(figsize=(5,5))\n",
    "            pylab.axis('off') \n",
    "            pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[])\n",
    "            librosa.display.specshow(log_power, cmap=cm.jet)\n",
    "            pylab.savefig(I_ADV_TEST_DIR + f[:-4]+'.jpg', bbox_inches=None, pad_inches=0)\n",
    "            pylab.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f, e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = 'spectrogram_images/train/'\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 16\n",
    "BATCH_SIZE = 10\n",
    "L2_LAMBDA = 0.001\n",
    "IMG_SIZE = (224, 224)\n",
    "WAV_DIR = 'data/unzipped/UrbanSound8K/audio/fold'\n",
    "IMG_DIR = 'spectrogram_images/train/'\n",
    "I_ADV_TRAIN_DIR = 'spectrogram_images/adv_train_in/'\n",
    "I_ADV_TEST_DIR = 'spectrogram_images/adv_test_in/'\n",
    "O_ADV_TRAIN_DIR = 'spectrogram_images/adv_train_out/'\n",
    "O_ADV_TEST_DIR = 'spectrogram_images/adv_test_out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = OneHotEncoder(categories=[range(NUM_CLASSES)])\n",
    "\n",
    "all_files = os.listdir(IMG_DIR)\n",
    "\n",
    "# Get class weights\n",
    "label_array = []\n",
    "for file_ in all_files:\n",
    "    vals = file_.split('-')[1]\n",
    "    label_array.append(int(vals))\n",
    "    \n",
    "cl_weight = compute_class_weight(class_weight = 'balanced', \n",
    "                                 classes = np.unique(label_array), \n",
    "                                 y = label_array)\n",
    "\n",
    "# Train-val-test split of files\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(all_files, \n",
    "                                                                      label_array,\n",
    "                                                                      random_state = 10, \n",
    "                                                                      test_size = 0.2\n",
    "                                                                     )\n",
    "\n",
    "# Among the test files, keep half for validation\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(test_files, test_labels,\n",
    "                                                                  random_state = 10, \n",
    "                                                                  test_size = 0.5\n",
    "                                                                 )\n",
    "\n",
    "adv_train_files = os.listdir(I_ADV_TRAIN_DIR)\n",
    "\n",
    "# Get class weights\n",
    "adv_train_label = []\n",
    "for file_ in all_files:\n",
    "    vals = file_.split('-')[1]\n",
    "    adv_train_label.append(int(vals))\n",
    "\n",
    "adv_test_files = os.listdir(I_ADV_TEST_DIR)\n",
    "\n",
    "# Get class weights\n",
    "adv_test_label = []\n",
    "for file_ in all_files:\n",
    "    vals = file_.split('-')[1]\n",
    "    adv_test_label.append(int(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = transforms.Compose([transforms.Resize(IMG_SIZE), transforms.ToTensor()])\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def image_loader(image_name):\n",
    "    \"\"\"load image, returns tensor\"\"\"\n",
    "    image = Image.open(image_name)\n",
    "    image = image.convert(\"RGB\")\n",
    "    image = loader(image).float()\n",
    "    image = normalize(image).float()\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_batch(file_list):\n",
    "    img_array = []\n",
    "    idx_array = []\n",
    "    label_array = []\n",
    "\n",
    "    for file_ in file_list:\n",
    "        im = IMG_DIR + file_\n",
    "        \n",
    "        img_array.append(np.array(image_loader(im)))\n",
    "\n",
    "        vals = file_.split('-')\n",
    "        idx_array.append(vals[0])\n",
    "        label_array.append([int(vals[1])])\n",
    "\n",
    "    label_array = one_hot.fit_transform(label_array).toarray()\n",
    "    img_array = torch.FloatTensor(img_array)\n",
    "    label_array = torch.LongTensor(label_array)\n",
    "    \n",
    "    return img_array, label_array\n",
    "\n",
    "def batch_generator(files, BATCH_SIZE):\n",
    "    L = len(files)\n",
    "\n",
    "    #this line is just to make the generator infinite, keras needs that    \n",
    "    while True:\n",
    "\n",
    "        batch_start = 0\n",
    "        batch_end = BATCH_SIZE\n",
    "\n",
    "        while batch_start < L:\n",
    "            \n",
    "            limit = min(batch_end, L)\n",
    "            file_list = files[batch_start: limit]\n",
    "            batch_img_array, batch_label_array = load_batch(file_list)\n",
    "\n",
    "            yield (batch_img_array, batch_label_array) # a tuple with two numpy arrays with batch_size samples     \n",
    "\n",
    "            batch_start += BATCH_SIZE   \n",
    "            batch_end += BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3)\n",
      "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Model Architecture Definition\n",
    "\n",
    "model = models.vgg16(pretrained=True) #load vgg model\n",
    "\n",
    "# you need to flatten the layer after vgg16 to get 18432, check the keras model in the original nb\n",
    "model.classifier = nn.Sequential(nn.Linear(512*7*7,512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.3),\n",
    "                                nn.Linear(512,NUM_CLASSES))  \n",
    "# note there is no softmax layer while training, validating and testing put in that operation manually as the learning\n",
    "# doesn't depend upon softmax layer, this way we will get logits. We can use logits in fgsm\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Validation Functions\n",
    "\n",
    "def train (model, loader, criterion):\n",
    "    #Custom Code for Progress Bar\n",
    "    bar = progressbar.ProgressBar(maxval=100, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    model.train()\n",
    "    current_loss = 0\n",
    "    current_correct = 0\n",
    "    i = 0\n",
    "    i_percentage = 0\n",
    "    for train, y_train in iter(loader):\n",
    "        i = i+train.shape[0]\n",
    "        if i > len(train_files):\n",
    "            break\n",
    "        y_train = torch.argmax(y_train, dim=1)\n",
    "        train, y_train = train.to(device), y_train.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(train)\n",
    "        _, preds = torch.max(output,1)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.classifier.parameters(), 50)\n",
    "        optimizer.step()\n",
    "        current_loss += loss.item()*train.size(0)\n",
    "        current_correct += torch.sum(preds == y_train.data)\n",
    "        i_percentage = (i/len(train_files))*100\n",
    "        bar.update(i_percentage)\n",
    "        \n",
    "    bar.finish()\n",
    "    epoch_loss = current_loss / len(train_files)\n",
    "    epoch_acc = current_correct.double() / len(train_files)\n",
    "        \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "#validate the model\n",
    "def validation (model, loader, criterion):\n",
    "    #Custom Code for Progress Bar\n",
    "    bar = progressbar.ProgressBar(maxval=100, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_correct = 0\n",
    "    i = 0\n",
    "    i_percentage = 0\n",
    "    for valid, y_valid in iter(loader):\n",
    "        i = i+valid.shape[0]\n",
    "        if i > len(val_files):\n",
    "            break\n",
    "        y_valid = torch.argmax(y_valid, dim=1)\n",
    "        valid, y_valid = valid.to(device), y_valid.to(device)\n",
    "        output = model.forward(valid)\n",
    "        valid_loss += criterion(output, y_valid).item()*valid.size(0)\n",
    "        equal = (output.max(dim=1)[1] == y_valid.data)\n",
    "        valid_correct += torch.sum(equal)#type(torch.FloatTensor)\n",
    "        i_percentage = (i/len(val_files))*100\n",
    "        bar.update(i_percentage)\n",
    "        \n",
    "    bar.finish()\n",
    "    epoch_loss = valid_loss / len(val_files)\n",
    "    epoch_acc = valid_correct.double() / len(val_files)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def testing (model, loader, criterion, test_size):\n",
    "    #Custom Code for Progress Bar\n",
    "    bar = progressbar.ProgressBar(maxval=100, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    i = 0\n",
    "    i_percentage = 0\n",
    "    for test, y_test in iter(loader):\n",
    "        i = i+test.shape[0]\n",
    "        if i > len(test_files):\n",
    "            break\n",
    "        y_test = torch.argmax(y_test, dim=1)\n",
    "        test, y_test = test.to(device), y_test.to(device)\n",
    "        output = model.forward(test)\n",
    "        test_loss += criterion(output, y_test).item()*test.size(0)\n",
    "        equal = (output.max(dim=1)[1] == y_test.data)\n",
    "        test_correct += torch.sum(equal)#type(torch.FloatTensor)\n",
    "        i_percentage = (i/test_size)*100\n",
    "        bar.update(i_percentage)\n",
    "        \n",
    "    bar.finish()\n",
    "    epoch_loss = test_loss / test_size\n",
    "    epoch_acc = test_correct.double() / test_size\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr = 0.005, momentum = 0.5)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss : 0.8729  Train Accuracy: 0.7070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Validation Loss : 0.5209  Validation Accuracy 0.8263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss : 0.3974  Train Accuracy: 0.8739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Validation Loss : 0.4271  Validation Accuracy 0.8588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss : 0.2277  Train Accuracy: 0.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Validation Loss : 0.4294  Validation Accuracy 0.8545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss : 0.1358  Train Accuracy: 0.9599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Validation Loss : 0.3662  Validation Accuracy 0.8955\n",
      "-1113.841768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#Train the model\n",
    "\n",
    "#freeze gradient parameters in pretrained model\n",
    "for param in model.parameters():\n",
    "    param.require_grad = True    \n",
    "\n",
    "#freeze gradient parameters in pretrained model\n",
    "for param in model.features.parameters():\n",
    "    param.require_grad = False\n",
    "    \n",
    "#train and validate\n",
    "#4 epochs seem to be enough\n",
    "epochs = 4\n",
    "epoch = 0\n",
    "\n",
    "start = time.clock()    \n",
    "for e in range(epochs):\n",
    "    epoch +=1\n",
    "    with torch.set_grad_enabled(True):\n",
    "        epoch_train_loss, epoch_train_acc = train(model,batch_generator(train_files,BATCH_SIZE),criteria)\n",
    "    print(\"Epoch: {} Train Loss : {:.4f}  Train Accuracy: {:.4f}\".format(epoch,epoch_train_loss,epoch_train_acc))\n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss, epoch_val_acc = validation(model,batch_generator(val_files,BATCH_SIZE),criteria)\n",
    "    print(\"Epoch: {} Validation Loss : {:.4f}  Validation Accuracy {:.4f}\".format(epoch,epoch_val_loss,epoch_val_acc))\n",
    "print(start - time.clock())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the original training model\n",
    "torch.save(model,'original_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('original_model.pt',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Test Loss : 0.4353  Test Accuracy 0.8559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Test Results\n",
    "\n",
    "with torch.no_grad():\n",
    "    epoch_test_loss, epoch_test_acc = testing(model,batch_generator(test_files,BATCH_SIZE),criteria,len(test_files))\n",
    "print(\"Epoch: {} Test Loss : {:.4f}  Test Accuracy {:.4f}\".format(1,epoch_test_loss,epoch_test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of model with adversarial examples is  5.15 %\n",
      "-420.3898868560791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_batch_2(file_list):\n",
    "    img_array = []\n",
    "    idx_array = []\n",
    "    label_array = []\n",
    "\n",
    "    for file_ in file_list:\n",
    "        im = I_ADV_TRAIN_DIR + file_\n",
    "        \n",
    "        img_array.append(np.array(image_loader(im)))\n",
    "\n",
    "        vals = file_.split('-')\n",
    "        idx_array.append(vals[0])\n",
    "        label_array.append([int(vals[1])])\n",
    "\n",
    "    label_array = one_hot.fit_transform(label_array).toarray()\n",
    "    img_array = torch.FloatTensor(img_array)\n",
    "    label_array = torch.LongTensor(label_array)\n",
    "    \n",
    "    return img_array, label_array\n",
    "\n",
    "\n",
    "#Generate Adversarial Images\n",
    "batch_size = 5\n",
    "pred = []\n",
    "adv_pred = []\n",
    "if 'adv_x' in locals():\n",
    "    del adv_x\n",
    "\n",
    "i=0\n",
    "bar = progressbar.ProgressBar(maxval=100, \\\n",
    "widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "bar.start()\n",
    "device = 'cpu'\n",
    "un_normalize = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],std=[1/0.229, 1/0.224, 1/0.225])\n",
    "un_convert = transforms.ToPILImage()\n",
    "\n",
    "# ADV_IMG_DIR = 'spectrogram_images/adv/'\n",
    "\n",
    "start = time.time()\n",
    "adv_size = len(adv_train_files)\n",
    "file_idx = 0\n",
    "while i<adv_size:\n",
    "    j = i + batch_size\n",
    "    if j>adv_size:\n",
    "        j = adv_size\n",
    "    x,y = load_batch_2(adv_train_files[i:j])\n",
    "    x = Variable(x, requires_grad=True)\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    model = model.to(device)\n",
    "    output = model.forward(x)\n",
    "    y_label = torch.argmax(y, dim=1)\n",
    "    y_label = y_label.to(device)\n",
    "    loss = criteria(output, y_label)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add perturbation \n",
    "    epsilon = 0.01\n",
    "    x_grad     = torch.sign(x.grad.data)\n",
    "    temp = x.data + epsilon*x_grad\n",
    "    if 'adv_x' in locals():\n",
    "        adv_x = torch.cat((adv_x,temp),0)\n",
    "    else:\n",
    "        adv_x = temp\n",
    "    adv_x = adv_x.to(device)\n",
    "    adv_output = model.forward(adv_x)\n",
    "    adv_output = adv_output.to(device)\n",
    "\n",
    "    x_pred = torch.argmax(y, dim=1)\n",
    "    x_adv_pred = torch.argmax(adv_output,dim=1)\n",
    "    \n",
    "    x_pred, x_adv_pred = x_pred.to(device), x_adv_pred.to(device)\n",
    "\n",
    "    pred.extend(x_pred.tolist())\n",
    "    adv_pred.extend(x_adv_pred.tolist())\n",
    "    for ii in range(j-i):\n",
    "        temp = adv_x[ii]\n",
    "        t1 = un_normalize(temp).float()\n",
    "        t2 = un_convert(t1)\n",
    "        t2.save(O_ADV_TRAIN_DIR + adv_train_files[file_idx], \"JPEG\", quality=80, optimize=True, progressive=True)\n",
    "        file_idx += 1\n",
    "        del temp\n",
    "        del t1\n",
    "        del t2\n",
    "    i = j\n",
    "    del adv_x\n",
    "    bar.update((i/adv_size)*100)\n",
    "\n",
    "bar.finish()\n",
    "\n",
    "comp = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == adv_pred[i]:\n",
    "        comp.extend([1])\n",
    "    else:\n",
    "        comp.extend([0])\n",
    "\n",
    "adv_acc = (sum(comp)/len(comp))*100\n",
    "print(\"The accuracy of model with adversarial examples is \",\"{0:.2f}\".format(adv_acc),\"%\")\n",
    "print(start - time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the files from training and adversarial images folder into a single training folder\n",
    "import shutil\n",
    "ADV_TRAIN_DIR = 'spectrogram_images/train_adv/'\n",
    "\n",
    "# for f in train_files:\n",
    "#     shutil.copy2(IMG_DIR + f,ADV_TRAIN_DIR)\n",
    "\n",
    "# for f in adv_train_files:\n",
    "#     shutil.copy2(O_ADV_TRAIN_DIR + f,ADV_TRAIN_DIR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the network now with original + adversarial images\n",
    "adv_files = os.listdir(ADV_TRAIN_DIR)\n",
    "\n",
    "# Get class weights\n",
    "adv_label = []\n",
    "for file_ in adv_files:\n",
    "    vals = file_.split('-')[1]\n",
    "    adv_label.append(int(vals))\n",
    "    \n",
    "cl_weight = compute_class_weight(class_weight = 'balanced', \n",
    "                                 classes = np.unique(label_array), \n",
    "                                 y = label_array)\n",
    "\n",
    "# Train-val-test split of files\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(adv_files, \n",
    "                                                                      adv_label,\n",
    "                                                                      random_state = 10, \n",
    "                                                                      test_size = 0.2\n",
    "                                                                     )\n",
    "\n",
    "# Among the test files, keep half for validation\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(test_files, test_labels,\n",
    "                                                                  random_state = 10, \n",
    "                                                                  test_size = 0.5\n",
    "                                                                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_train (model, loader, criterion):\n",
    "    #Custom Code for Progress Bar\n",
    "    bar = progressbar.ProgressBar(maxval=100, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    \n",
    "    model.train()\n",
    "    current_loss = 0\n",
    "    current_correct = 0\n",
    "    i = 0\n",
    "    i_percentage = 0\n",
    "    for train, y_train in iter(loader):\n",
    "        train, y_train = train.to(device), y_train.to(device)\n",
    "        train = Variable(train, requires_grad=True)\n",
    "        i = i+train.shape[0]\n",
    "        if i > len(train_files):\n",
    "            break\n",
    "        y_train = torch.argmax(y_train, dim=1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(train)\n",
    "        _, preds = torch.max(output,1)\n",
    "        loss = criterion(output, y_train)\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "#         #adversarial\n",
    "        \n",
    "#         output = model.forward(x)\n",
    "#         y_label = torch.argmax(y, dim=1)\n",
    "#         loss = criteria(output, y_label)\n",
    "#         loss.backward()\n",
    "\n",
    "        # Add perturbation \n",
    "        epsilon = 0.01\n",
    "        train_grad = torch.sign(train.grad.data)\n",
    "        adv_train = train.data + epsilon*train_grad\n",
    "        adv_train = adv_train.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        adv_output = model.forward(adv_train)\n",
    "        adv_output = adv_output.to(device)\n",
    "        _, preds = torch.max(adv_output,1)\n",
    "#        UNCOMMENT THIS LINE FOR NEXT RUN *************************\n",
    "#       loss = 0.88 * (loss) + 0.12 * (criterion(adv_output, y_train))\n",
    "        loss = 0.6 * (loss) + 0.4 * (criterion(adv_output, y_train))\n",
    "        loss.backward()\n",
    "        \n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.classifier.parameters(), 50)\n",
    "        optimizer.step()\n",
    "        current_loss += loss.item()*train.size(0)\n",
    "        current_correct += torch.sum(preds == y_train.data)\n",
    "        i_percentage = (i/len(train_files))*100\n",
    "        bar.update(i_percentage)\n",
    "        \n",
    "    bar.finish()\n",
    "    epoch_loss = current_loss / len(train_files)\n",
    "    epoch_acc = current_correct.double() / len(train_files)\n",
    "        \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Model Architecture Definition\n",
    "\n",
    "model = models.vgg16(pretrained=True) #load vgg model\n",
    "\n",
    "# you need to flatten the layer after vgg16 to get 18432, check the keras model in the original nb\n",
    "model.classifier = nn.Sequential(nn.Linear(512*7*7,512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.3),\n",
    "                                nn.Linear(512,NUM_CLASSES))  \n",
    "# note there is no softmax layer while training, validating and testing put in that operation manually as the learning\n",
    "# doesn't depend upon softmax layer, this way we will get logits. We can use logits in fgsm\n",
    "\n",
    "criteria = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr = 0.005, momentum = 0.5)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "#freeze gradient parameters in pretrained model\n",
    "for param in model.parameters():\n",
    "    param.require_grad = True    \n",
    "\n",
    "#freeze gradient parameters in pretrained model\n",
    "for param in model.features.parameters():\n",
    "    param.require_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_v(file_list):\n",
    "    img_array = []\n",
    "    idx_array = []\n",
    "    label_array = []\n",
    "\n",
    "    for file_ in file_list:\n",
    "        im = ADV_TRAIN_DIR + file_\n",
    "        \n",
    "        img_array.append(np.array(image_loader(im)))\n",
    "\n",
    "        vals = file_.split('-')\n",
    "        idx_array.append(vals[0])\n",
    "        label_array.append([int(vals[1])])\n",
    "\n",
    "    label_array = one_hot.fit_transform(label_array).toarray()\n",
    "    img_array = torch.FloatTensor(img_array)\n",
    "    label_array = torch.LongTensor(label_array)\n",
    "    \n",
    "    return img_array.to(device), label_array.to(device)\n",
    "\n",
    "def batch_generator_v(files, BATCH_SIZE):\n",
    "    L = len(files)\n",
    "\n",
    "    #this line is just to make the generator infinite, keras needs that    \n",
    "    while True:\n",
    "\n",
    "        batch_start = 0\n",
    "        batch_end = BATCH_SIZE\n",
    "\n",
    "        while batch_start < L:\n",
    "            \n",
    "            limit = min(batch_end, L)\n",
    "            file_list = files[batch_start: limit]\n",
    "            batch_img_array, batch_label_array = load_batch_v(file_list)\n",
    "\n",
    "            yield (batch_img_array, batch_label_array) # a tuple with two numpy arrays with batch_size samples     \n",
    "\n",
    "            batch_start += BATCH_SIZE   \n",
    "            batch_end += BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss : 1.4621  Train Accuracy: 0.2650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Validation Loss : 0.7803  Validation Accuracy 0.7465\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss : 1.0687  Train Accuracy: 0.3676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Validation Loss : 0.5885  Validation Accuracy 0.8213\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss : 0.8842  Train Accuracy: 0.4333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Validation Loss : 0.5069  Validation Accuracy 0.8416\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss : 0.7522  Train Accuracy: 0.4827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Validation Loss : 0.4369  Validation Accuracy 0.8669\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Loss : 0.6470  Train Accuracy: 0.5279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Validation Loss : 0.3743  Validation Accuracy 0.8847\n",
      "-1539.211900472641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#train and validate\n",
    "#Need to see at what epoch gain stops\n",
    "epochs = 20\n",
    "epoch = 0\n",
    "\n",
    "start = time.time()\n",
    "for e in range(epochs):\n",
    "    epoch +=1\n",
    "    print(epoch)\n",
    "    with torch.set_grad_enabled(True):\n",
    "        epoch_train_loss, epoch_train_acc = adv_train(model,batch_generator_v(train_files,BATCH_SIZE),criteria)\n",
    "    print(\"Epoch: {} Train Loss : {:.4f}  Train Accuracy: {:.4f}\".format(epoch,epoch_train_loss,epoch_train_acc))\n",
    "    with torch.no_grad():\n",
    "        epoch_val_loss, epoch_val_acc = validation(model,batch_generator_v(val_files,BATCH_SIZE),criteria)\n",
    "    print(\"Epoch: {} Validation Loss : {:.4f}  Validation Accuracy {:.4f}\".format(epoch,epoch_val_loss,epoch_val_acc))\n",
    "print(start - time.time())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'adv_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adv = torch.load('adv_model.pt',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Test Loss : 0.4328  Test Accuracy 0.8671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Test Results\n",
    "\n",
    "with torch.no_grad():\n",
    "    epoch_test_loss, epoch_test_acc = testing(model,batch_generator_v(test_files,BATCH_SIZE),criteria,len(test_files))\n",
    "print(\"Epoch: {} Test Loss : {:.4f}  Test Accuracy {:.4f}\".format(1,epoch_test_loss,epoch_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of model with adversarial examples is  22.22 %\n",
      "-33.422019481658936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_batch_3(file_list):\n",
    "    img_array = []\n",
    "    idx_array = []\n",
    "    label_array = []\n",
    "\n",
    "    for file_ in file_list:\n",
    "        im = I_ADV_TEST_DIR + file_\n",
    "        \n",
    "        img_array.append(np.array(image_loader(im)))\n",
    "\n",
    "        vals = file_.split('-')\n",
    "        idx_array.append(vals[0])\n",
    "        label_array.append([int(vals[1])])\n",
    "\n",
    "    label_array = one_hot.fit_transform(label_array).toarray()\n",
    "    img_array = torch.FloatTensor(img_array)\n",
    "    label_array = torch.LongTensor(label_array)\n",
    "    \n",
    "    return img_array.to(device), label_array.to(device)\n",
    "\n",
    "start = time.time()\n",
    "#Generate Adversarial Images\n",
    "batch_size = 5\n",
    "pred = []\n",
    "adv_pred = []\n",
    "if 'adv_x' in locals():\n",
    "    del adv_x\n",
    "\n",
    "i=0\n",
    "bar = progressbar.ProgressBar(maxval=100, \\\n",
    "widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "bar.start()\n",
    "\n",
    "un_normalize = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],std=[1/0.229, 1/0.224, 1/0.225])\n",
    "un_convert = transforms.ToPILImage()\n",
    "\n",
    "# ADV_IMG_DIR = 'spectrogram_images/adv/'\n",
    "\n",
    "\n",
    "adv_size = len(adv_test_files)\n",
    "file_idx = 0\n",
    "while i<adv_size:\n",
    "    j = i + batch_size\n",
    "    if j>adv_size:\n",
    "        j = adv_size\n",
    "    x,y = load_batch_3(adv_test_files[i:j])\n",
    "    x = Variable(x, requires_grad=True)\n",
    "    model = model.to(device)\n",
    "    output = model.forward(x)\n",
    "    y_label = torch.argmax(y, dim=1)\n",
    "    loss = criteria(output, y_label)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add perturbation \n",
    "    epsilon = 0.01\n",
    "    x_grad     = torch.sign(x.grad.data)\n",
    "    temp = x.data + epsilon*x_grad\n",
    "    if 'adv_x' in locals():\n",
    "        adv_x = torch.cat((adv_x,temp),0)\n",
    "    else:\n",
    "        adv_x = temp\n",
    "    adv_x = adv_x.to(device)\n",
    "    adv_output = model.forward(adv_x)\n",
    "\n",
    "    x_pred = torch.argmax(y, dim=1)\n",
    "    x_adv_pred = torch.argmax(adv_output,dim=1)\n",
    "    \n",
    "\n",
    "    pred.extend(x_pred.tolist())\n",
    "    adv_pred.extend(x_adv_pred.tolist())\n",
    "    for ii in range(j-i):\n",
    "        temp = adv_x[ii]\n",
    "        temp = temp.to('cpu')\n",
    "        t1 = un_normalize(temp).float()\n",
    "        t2 = un_convert(t1)\n",
    "        t2.save(O_ADV_TEST_DIR + adv_test_files[file_idx], \"JPEG\", quality=80, optimize=True, progressive=True)\n",
    "        file_idx += 1\n",
    "        del temp\n",
    "        del t1\n",
    "        del t2\n",
    "    i = j\n",
    "    del adv_x\n",
    "    bar.update((i/adv_size)*100)\n",
    "\n",
    "bar.finish()\n",
    "\n",
    "comp = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == adv_pred[i]:\n",
    "        comp.extend([1])\n",
    "    else:\n",
    "        comp.extend([0])\n",
    "\n",
    "adv_acc = (sum(comp)/len(comp))*100\n",
    "print(\"The accuracy of model with adversarial examples is \",\"{0:.2f}\".format(adv_acc),\"%\")\n",
    "print(start - time.time()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
